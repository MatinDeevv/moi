# Project ME v0.2

**Local Automation & Orchestration System**

A powerful, user-in-the-loop automation platform that runs entirely on your local machine, powered by Python and LM Studio.

---

## âœ¨ What is Project ME?

Project ME is a **local-first automation orchestrator** that combines:
- âœ… **Task Management** - Create, queue, and execute automation tasks
- âœ… **LLM Integration** - Local AI via LM Studio (no cloud APIs)
- âœ… **Multiple Interfaces** - CLI, REST API, and Web UI
- âœ… **Persistent Memory** - JSONL-based storage with full event logging
- âœ… **Session Support** - Conversational LLM sessions with rolling summaries
- âœ… **Tool Ecosystem** - Shell, filesystem, code analysis, and more

**New in v0.2:**
- ğŸš€ Full REST API (FastAPI)
- ğŸ¨ Modern Web UI (Next.js + TailwindCSS)
- ğŸ’¬ LLM session support with conversation history
- ğŸ“Š Enhanced event tracking and filtering

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Node.js 18+ (for Web UI only)
- LM Studio (for LLM features)

### Installation

```bash
# Clone the repo
git clone https://github.com/MatinDeevv/moi.git
cd moi

# Install Python dependencies
pip install -r requirements.txt

# (Optional) Install Web UI dependencies
cd app
npm install
cd ..
```

### Running the System

**Option 1: CLI Mode**
```bash
python main.py
```

**Option 2: API Server**
```bash
python main.py --api
# Or: start_api.bat (Windows)
```

**Option 3: Full Stack (API + Web UI)**
```bash
# Terminal 1
start_api.bat

# Terminal 2
start_web.bat

# Open http://localhost:3000
```

ğŸ“– **Full Guide:** See [docs/COMPLETE_STARTUP_GUIDE.md](docs/COMPLETE_STARTUP_GUIDE.md)

---

## ğŸ¯ Features

### Task Types
- **Shell** - Execute system commands
- **Generic LLM** - Ask questions to local AI
- **Filesystem** - Read/write/list files
- **Code Analysis** - Analyze code with LLM
- **LLM Session** - Multi-turn conversations with context

### Interfaces

| Feature | CLI | API | Web UI |
|---------|-----|-----|--------|
| Create Tasks | âœ… | âœ… | âœ… |
| Run Tasks | âœ… | âœ… | âœ… |
| View History | âœ… | âœ… | âœ… |
| Filters & Search | âœ… | âœ… | âœ… |
| Real-time Updates | âŒ | âŒ | ğŸ”„ Manual |

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI    â”‚ â† Next.js (http://localhost:3000)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  REST API   â”‚ â† FastAPI (http://localhost:8000)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Business Logic         â”‚ â† Python modules
â”‚  â”œâ”€â”€ TaskStore          â”‚
â”‚  â”œâ”€â”€ Agent (orchestrator)â”‚
â”‚  â”œâ”€â”€ Memory & Events    â”‚
â”‚  â””â”€â”€ Tools (shell, fs)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ JSONL Files â”‚ â† logs/tasks.jsonl, logs/events.jsonl
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
moi/
â”œâ”€â”€ main.py                 # CLI entrypoint + --api flag
â”œâ”€â”€ api_server.py           # FastAPI server (NEW in v0.2)
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ start_api.bat          # Quick-start API server
â”œâ”€â”€ start_web.bat          # Quick-start Web UI
â”‚
â”œâ”€â”€ src/                   # Core business logic
â”‚   â”œâ”€â”€ agent.py           # Task orchestrator
â”‚   â”œâ”€â”€ tasks.py           # Task management
â”‚   â”œâ”€â”€ memory.py          # Events & sessions
â”‚   â”œâ”€â”€ llm_client.py      # LM Studio wrapper
â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â””â”€â”€ tools/             # Tool modules
â”‚
â”œâ”€â”€ logs/                  # Persistent storage (JSONL)
â”‚   â”œâ”€â”€ tasks.jsonl
â”‚   â””â”€â”€ events.jsonl
â”‚
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ COMPLETE_STARTUP_GUIDE.md
â”‚   â”œâ”€â”€ API_WEB_UI_GUIDE.md
â”‚   â””â”€â”€ PHASE_v0.2_SUMMARY.md
â”‚
â””â”€â”€ app/                   # Next.js Web UI (NEW in v0.2)
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ page.tsx       # Dashboard
    â”‚   â”œâ”€â”€ layout.tsx     # Root layout
    â”‚   â””â”€â”€ components/    # UI components
    â”œâ”€â”€ lib/
    â”‚   â””â”€â”€ api.ts         # API client
    â””â”€â”€ package.json
```

---

## ğŸ’¡ Examples

### Example 1: Simple Shell Task (CLI)
```bash
python main.py
# Menu: 1 (Create task) â†’ 1 (Shell)
# Command: dir
# Menu: 2 (Run next task)
```

### Example 2: LLM Session (CLI)
```bash
python main.py
# Menu: 9 (Create LLM session)
# Session ID: my-chat
# Message: "Hello, how are you?"
# Menu: 2 (Run task)
# Menu: 9 (Continue conversation)
```

### Example 3: Create Task via API
```bash
curl -X POST http://localhost:8000/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "type": "shell",
    "payload": {"command": "echo Hello World"},
    "title": "Test task"
  }'
```

### Example 4: Web UI Workflow
1. Open http://localhost:3000
2. Click "â• Create Task"
3. Select "Shell Command"
4. Fill in payload: `{"command": "dir"}`
5. Click "Create Task"
6. Click "â–¶ Run Next Task"
7. View results in real-time

---

## ğŸš€ Deployment

### Local Development (Recommended)
```bash
# CLI Mode
python main.py

# Full Stack Mode
start_api.bat  # Terminal 1
start_web.bat  # Terminal 2
```

### Production Deployment

**âš ï¸ Important:** The Next.js Web UI and Python FastAPI backend must be deployed separately.

**Option 1: Vercel (Web UI) + Railway (Backend)**
```bash
# 1. Deploy backend to Railway
npm install -g @railway/cli
railway login
railway init
railway up

# 2. Deploy frontend to Vercel
cd app
npm install -g vercel
vercel
# Set NEXT_PUBLIC_API_URL to your Railway URL
```

**Option 2: Self-Hosted (VPS)**
- Both frontend and backend on same server
- Use nginx for reverse proxy
- SSL with Let's Encrypt

ğŸ“– **Full Deployment Guide:** [docs/VERCEL_DEPLOYMENT.md](docs/VERCEL_DEPLOYMENT.md)

---

## ğŸ”§ Configuration

### LM Studio Setup (for LLM features)

1. Download LM Studio: https://lmstudio.ai/
2. Load a model (e.g., Llama 2, Mistral, etc.)
3. Start server (Server tab â†’ Start)
4. Verify: http://localhost:1234/v1/models

Edit `src/config.py`:
```python
LLM_BASE_URL = "http://localhost:1234/v1"
LLM_MODEL_NAME = "local-model"
```

### API Port Configuration

Edit `api_server.py`:
```python
def start_server(host: str = "0.0.0.0", port: int = 8000):
    # Change port if needed
```

---

## ğŸ“š Documentation

- **[Complete Startup Guide](docs/COMPLETE_STARTUP_GUIDE.md)** - Installation, setup, troubleshooting
- **[API & Web UI Guide](docs/API_WEB_UI_GUIDE.md)** - API endpoints, Web UI features
- **[Phase v0.2 Summary](docs/PHASE_v0.2_SUMMARY.md)** - What's new in v0.2
- **[Commands Reference](docs/COMMANDS.md)** - CLI commands
- **[Folder Structure](docs/FOLDER_STRUCTURE.md)** - Project organization

**API Documentation (Interactive):**
- Start API server: `python main.py --api`
- Visit: http://localhost:8000/docs

---

## ğŸ› Troubleshooting

### Common Issues

**API won't start:**
```bash
pip install -r requirements.txt
python -c "import fastapi, uvicorn; print('OK')"
```

**Web UI can't connect:**
1. Ensure API is running: http://localhost:8000/health
2. Check browser console for errors
3. Verify CORS is enabled in `api_server.py`

**LLM tasks fail:**
1. Start LM Studio
2. Load a model
3. Start the server
4. Test: http://localhost:1234/v1/models

ğŸ“– **Full troubleshooting:** [docs/COMPLETE_STARTUP_GUIDE.md#troubleshooting](docs/COMPLETE_STARTUP_GUIDE.md#troubleshooting)

---

## ğŸ›£ï¸ Roadmap

### âœ… Completed (v0.2)
- [x] REST API layer (FastAPI)
- [x] Web UI (Next.js)
- [x] LLM sessions with rolling summaries
- [x] Enhanced task filtering
- [x] Detailed event tracking

### ğŸ”œ Coming Soon (v0.3)
- [ ] Real-time WebSocket updates
- [ ] Task scheduling (cron-like)
- [ ] Session chat UI
- [ ] Authentication & multi-user
- [ ] Desktop automation tools (PyAutoGUI)

### ğŸ”® Future
- [ ] Plugin system
- [ ] Trading/backtesting modules
- [ ] Browser automation (Playwright)
- [ ] Computer vision tools (OCR, screenshot analysis)

---

## ğŸ”’ Security Note

âš ï¸ **This is a LOCAL development tool.**

- **DO NOT** expose to the internet without authentication
- **DO NOT** run untrusted shell commands
- **DO** use firewall rules to restrict access
- **DO** consider using VPN (Tailscale) for remote access

---

## ğŸ¤ Contributing

Contributions welcome! When adding features:
1. Follow existing patterns (thin API wrappers, business logic in `src/`)
2. Update documentation
3. Add examples
4. Test in CLI, API, and Web UI modes

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- **LM Studio** - Local LLM inference
- **FastAPI** - Modern Python API framework
- **Next.js** - React framework
- **TailwindCSS** - Utility-first CSS

---

## ğŸ“ Links

- **GitHub:** https://github.com/MatinDeevv/moi
- **Documentation:** [docs/](docs/)
- **Issues:** https://github.com/MatinDeevv/moi/issues

---

**Built with â¤ï¸ for local-first automation**

**Version:** v0.2.0  
**Last Updated:** December 1, 2025

