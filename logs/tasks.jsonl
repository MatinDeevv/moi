{"id": "5837b454-2d1c-448c-8228-1f92a24aa2ce", "type": "shell", "payload": {"command": "echo test"}, "status": "done", "created_at": "2025-11-30T01:56:16.218788", "updated_at": "2025-11-30T02:04:01.175752", "result": {"success": true, "output": "test\n", "error": null}, "error": null}
{"id": "7495a432-85c0-4f08-9586-f7b419604b77", "type": "shell", "payload": {"command": "Get-Date"}, "status": "done", "created_at": "2025-11-30T01:57:59.892153", "updated_at": "2025-11-30T01:58:00.343597", "result": {"success": true, "output": "\nSaturday, November 29, 2025 8:58:00 PM\n\n\n", "error": null}, "error": null}
{"id": "effed6da-52f3-4714-80fb-d0b0174042cc", "type": "filesystem", "payload": {"operation": "read", "filepath": "README.md"}, "status": "done", "created_at": "2025-11-30T01:58:00.345953", "updated_at": "2025-11-30T01:58:00.351788", "result": {"success": true, "content": "# Project ME v0\n\n**Local Automation & Orchestration System**\n\nA Python-based task orchestration system that runs locally on Windows, using LM Studio for local LLM inference.\n\n## Overview\n\nProject ME v0 is a user-in-the-loop automation system that:\n- Manages tasks with JSONL persistence\n- Executes shell commands, filesystem operations, and code analysis\n- Uses LM Studio (local OpenAI-compatible endpoint) for intelligent task processing\n- Logs all events for debugging and history\n- Operates in single-step mode (no autonomous infinite loops)\n\n## Architecture\n\n```\nProject ME v0\n\u251c\u2500\u2500 main.py              # CLI entrypoint\n\u251c\u2500\u2500 config.py            # Configuration settings\n\u251c\u2500\u2500 tasks.py             # Task dataclass + JSONL persistence\n\u251c\u2500\u2500 memory.py            # Event logging system\n\u251c\u2500\u2500 llm_client.py        # LM Studio wrapper\n\u251c\u2500\u2500 agent.py             # Core orchestrator logic\n\u2514\u2500\u2500 tools/               # Tool modules\n    \u251c\u2500\u2500 __init__.py      # Tool registry\n    \u251c\u2500\u2500 shell_tools.py   # Shell command execution\n    \u2514\u2500\u2500 fs_tools.py      # Filesystem operations\n```\n\n## Setup\n\n1. **Install dependencies:**\n   ```powershell\n   pip install -r requirements.txt\n   ```\n\n2. **Start LM Studio:**\n   - Launch LM Studio\n   - Load a model (e.g., gpt-oss:20b or any compatible model)\n   - Start the local server (default: http://localhost:1234)\n\n3. **Run Project ME:**\n   ```powershell\n   python main.py\n   ```\n\n## Usage\n\n### Main Menu Options\n\n1. **Create a new task** - Define tasks of various types:\n   - Shell: Execute Windows PowerShell commands\n   - Generic LLM: Ask questions to the local LLM\n   - Filesystem: Read/write/list files and directories\n   - Code Analysis: Analyze code files with LLM assistance\n\n2. **Run next pending task** - Process the next task in queue (FIFO order)\n\n3. **View recent tasks** - See task history and status\n\n4. **View recent events** - See detailed event logs\n\n5. **List available tools** - Show all registered tool functions\n\n### Example Workflows\n\n**Execute a shell command:**\n1. Choose option 1 (Create task)\n2. Select task type 1 (shell)\n3. Enter command: `Get-ChildItem`\n4. Choose option 2 (Run next task)\n5. Confirm execution\n\n**Ask the LLM a question:**\n1. Choose option 1 (Create task)\n2. Select task type 2 (generic_llm)\n3. Enter your prompt\n4. Choose option 2 (Run next task)\n\n**Read a file:**\n1. Choose option 1 (Create task)\n2. Select task type 3 (filesystem)\n3. Select operation 1 (read)\n4. Enter file path\n5. Choose option 2 (Run next task)\n\n## Data Storage\n\nAll data is stored locally in the `logs/` directory:\n- `logs/tasks.jsonl` - Task records (one JSON object per line)\n- `logs/events.jsonl` - Event logs (one JSON object per line)\n\nThese files persist across sessions and can be inspected/analyzed manually.\n\n## Configuration\n\nEdit `config.py` to customize:\n- LM Studio endpoint URL\n- Model name\n- Temperature and token limits\n- Timeout settings\n- Output truncation limits\n\n## Safety Features\n\nProject ME v0 is designed with safety in mind:\n- **No autonomous loops**: Each task requires explicit user confirmation\n- **User-in-the-loop**: CLI-driven execution model\n- **Comprehensive logging**: All actions are logged to disk\n- **Bounded execution**: Shell commands have timeouts\n- **Output limits**: Large outputs are truncated to prevent memory issues\n\n## Extending the System\n\n### Adding New Tools\n\n1. Create a new function in `tools/` directory\n2. Use the `@register_tool(\"tool_name\")` decorator\n3. Follow the pattern: accept parameters, log events, return dict with success status\n\nExample:\n```python\nfrom tools import register_tool\nfrom memory import memory, EventType\n\n@register_tool(\"my_custom_tool\")\ndef my_custom_tool(param: str, task_id: str = None) -> dict:\n    memory.log_event(EventType.TOOL_CALLED, {\"tool\": \"my_custom_tool\"}, task_id)\n    # ... do work ...\n    return {\"success\": True, \"result\": \"...\"}\n```\n\n### Adding New Task Types\n\n1. Add a new task type to `TaskType` enum in `tasks.py`\n2. Implement a handler method in `agent.py` (e.g., `_handle_my_task`)\n3. Add routing logic in `agent.process_task()`\n4. Update the CLI in `main.py` to support creating this task type\n\n## Troubleshooting\n\n**LM Studio connection errors:**\n- Ensure LM Studio is running\n- Check that the server is started (look for the green indicator)\n- Verify the endpoint URL in `config.py` matches your LM Studio settings\n\n**Task failures:**\n- Check `logs/events.jsonl` for detailed error information\n- View recent events from the main menu (option 4)\n\n**Shell command timeouts:**\n- Increase `SHELL_TIMEOUT_SECONDS` in `config.py`\n- Break long-running tasks into smaller steps\n\n## License\n\nInternal tool for personal use.\n\n## Version\n\nv0 - Initial implementation (November 2025)\n\n", "error": null}, "error": null}
{"id": "8318bdff-3bd2-4ab0-bea8-322657828f27", "type": "generic_llm", "payload": {"prompt": "What is the capital of France? Answer in one word.", "system_prompt": "You are a helpful assistant that gives concise answers."}, "status": "done", "created_at": "2025-11-30T01:58:00.354543", "updated_at": "2025-11-30T01:58:06.994459", "result": {"success": true, "response": "Paris"}, "error": null}
{"id": "39630abc-2040-4d6d-987f-9a8ba8dc24b5", "type": "filesystem", "payload": {"operation": "read", "filepath": "\"C:\\Users\\matin\\OneDrive\\Desktop\\hellotestllm.txt\""}, "status": "done", "created_at": "2025-11-30T02:03:38.743909", "updated_at": "2025-11-30T02:04:07.624017", "result": {"success": false, "content": "", "error": "File not found: \"C:\\Users\\matin\\OneDrive\\Desktop\\hellotestllm.txt\""}, "error": null}
{"id": "0705bdbd-095b-4ca8-af33-08c363febbc6", "type": "generic_llm", "payload": {"prompt": "hello llm if you can see this just say meow with a cat emoji in response", "system_prompt": "hello llm if you can see this just say meow with a cat emoji in response"}, "status": "done", "created_at": "2025-11-30T02:04:38.834917", "updated_at": "2025-11-30T02:04:53.589597", "result": {"success": true, "response": "Meow \ud83d\udc31"}, "error": null}
{"id": "02a610a0-7e0d-4d1a-a729-49c3215d8793", "type": "shell", "payload": {"command": "echo test"}, "status": "pending", "created_at": "2025-11-30T02:12:31.866071", "updated_at": "2025-11-30T02:12:31.866081", "result": null, "error": null, "title": "Test v0.1", "tags": ["test", "v0.1"]}
